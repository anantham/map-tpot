# TPOT Community Graph Analyzer

Python-based network analysis toolkit for exploring the TPOT (This Part of Twitter) community using the Community Archive dataset (6.9M tweets, 12.3M likes, follow relationships, and detailed profile metadata).

## Project Status

**Phase 1.1 (Complete):** Data pipeline with cached Supabase access
**Phase 1.2 (Complete):** Follow graph construction, status metrics (PageRank, Louvain, betweenness), CLI analysis tool
**Phase 1.3 (Complete):** Interactive visualization with React + d3-force graph explorer
**Phase 1.4 (In Progress):** Shadow enrichment pipeline with policy-driven refresh logic

See [docs/WORKLOG.md](./docs/WORKLOG.md) for detailed progress and [docs/adr/](./docs/adr/) for architectural decisions.

## Data Snapshot

<!-- AUTO:GRAPH_SNAPSHOT -->
_Directed graph snapshot pending â€” run `python -m scripts.analyze_graph --include-shadow --update-readme` to populate this section._
<!-- /AUTO:GRAPH_SNAPSHOT -->

## Prerequisites

- Python 3.9+
- Supabase anon key for the Community Archive (public read-only access)
- ~50MB disk space for SQLite cache

## Setup

1. **Create virtual environment and install dependencies:**

```bash
python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

2. **Configure environment variables:**

Create a `.env` file in the project root or export variables:

```bash
SUPABASE_URL=https://fabxmporizzqflnftavs.supabase.co  # Default Community Archive endpoint
SUPABASE_KEY=your_anon_key_here                        # Required: anon key for read access
CACHE_DB_PATH=./data/cache.db                          # Optional: defaults to ./data/cache.db
CACHE_MAX_AGE_DAYS=7                                   # Optional: defaults to 7 days
```

See `.env.example` for a template.

3. **Verify setup:**

```bash
python3 scripts/verify_setup.py
```

Expected output:
- âœ“ Supabase connection successful
- âœ“ Found 280+ profiles in Community Archive
- âœ“ Cache initialized at `data/cache.db`
- Sample accounts list

## Architecture

### Data Pipeline (Phase 1.1)

The analyzer uses an API-first architecture backed by local SQLite caching:

- **`src/config.py`**: Environment configuration loader for Supabase credentials and cache settings
- **`src/data/fetcher.py`**: `CachedDataFetcher` class providing pandas DataFrames for:
  - `fetch_profiles()` â€” account profile metadata
  - `fetch_accounts()` â€” account-level stats (followers, tweet counts)
  - `fetch_followers()` â€” follower edges
  - `fetch_following()` â€” following edges
  - `fetch_tweets()` â€” tweet content and metadata
  - `fetch_likes()` â€” like interactions

All data is cached in SQLite for 7 days (configurable). Cache expires automatically and refreshes from Supabase when stale.

### Graph Analysis (Phase 1.2 â€” Complete)

Features:
- Directed follow graph construction with mutual-only view support
- Baseline metrics: Personalized PageRank, Louvain communities, betweenness centrality
- CLI analysis tool (`scripts/analyze_graph.py`) with JSON output
- Configurable seed selection via presets (`docs/seed_presets.json`)
- Support for shadow enrichment integration (`--include-shadow` flag)

See [ADR 002](./docs/adr/002-graph-analysis-foundation.md) for full specification.

### Interactive Visualization (Phase 1.3 â€” Complete)

The graph explorer (`graph-explorer/`) is a React + Vite application using d3-force for interactive network visualization:

- Force-directed layout with configurable physics parameters
- Real-time metrics display (PageRank, betweenness, community assignments)
- Shadow enrichment toggle to show/hide expanded network
- Node filtering, edge styling, and interactive controls
- Persistent parameter state and export capabilities

Run the explorer:
```bash
cd graph-explorer
npm install
npm run dev
```

Generate analysis data:
```bash
python -m scripts.analyze_graph --include-shadow --output graph-explorer/public/analysis_output.json
```

#### macOS launcher

On macOS you can automate the dual-terminal startup with `StartGraphExplorer.command`:
1. `chmod +x StartGraphExplorer.command` (one-time to mark it executable)
2. Double-click the file in Finder or run `./StartGraphExplorer.command`

The script opens Terminal windows for the Flask API (`scripts.start_api_server`) and the Vite dev server, then launches your browser at `http://localhost:5173`.

### Shadow Enrichment Pipeline (Phase 1.4 â€” In Progress)

Shadow enrichment expands the graph beyond Community Archiveâ€™s 275 indexed accounts by scraping follow/follower lists with Selenium and backfilling metadata via the X API v2 when available. Data lives in dedicated `shadow_account` / `shadow_edge` tables so provenance stays explicit.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ scripts/enrich_shadowâ”‚  --cookies, --preview-count, --auto-confirm-first
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SeleniumWorker      â”‚â†’â†’â”‚ followers_you_follow â”‚ (optional, on by default)
â”‚  - captures lists    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  - reads profile tab â”‚
â”‚  - samples bios      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Human confirmation   â”‚  preview top N (default 10) with coverage ratios
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ HybridShadowEnricher â”‚  merges captures, resolves IDs, writes shadow tables
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

During the confirmation gate youâ€™ll see:
- Seed profile metadata (display name, bio, location, website, claimed counts)
- Coverage per list (`captured / claimed` with percentage)
- A preview of the first *N* profiles (set with `--preview-count`, default 10)

Use `--auto-confirm-first` only after verifying the preview looks accurate. Disable the reciprocal list scrape with `--no-followers-you-follow` if you need to minimise requests.

Core components:
- **`scripts/enrich_shadow_graph.py`** â€” runs the hybrid pipeline (cookies + optional bearer token required)
- **`src/shadow/`** â€” Selenium worker, X API client, and enrichment coordinator
- **`src/data/shadow_store.py`** â€” typed access to the shadow cache tables
- **`scripts/verify_shadow_graph.py`** â€” reports âœ“/âœ— status for cached shadow data and analysis output

`scripts/enrich_shadow_graph.py` launches Chrome in visible mode by default and applies human-like delays (4â€“9â€¯s) between actions. Use `--headless` only if you know Xâ€™s UI renders correctly without an interactive browser; pass `--chrome-binary` when you want to point at a custom Chrome install.

Example run:

```bash
# Setup cookies (one-time)
python -m scripts.setup_cookies --output secrets/twitter_cookies.pkl

# Run enrichment (default: fast, no API fallback)
python -m scripts.enrich_shadow_graph \
  --center adityaarpitha \
  --skip-if-ever-scraped \
  --auto-confirm-first \
  --max-scrolls 20

# With API fallback (slower but enriches accounts with missing bios)
python -m scripts.enrich_shadow_graph \
  --enable-api-fallback \
  --bearer-token "$X_BEARER_TOKEN" \
  --center adityaarpitha

# For long-running enrichment (macOS): prevent system sleep with caffeinate
caffeinate -disu .venv/bin/python3 -m scripts.enrich_shadow_graph --center adityaarpitha

# Analyze and verify
python -m scripts.analyze_graph --include-shadow --output analysis_output.json
python -m scripts.verify_shadow_graph
```

**Key flags:**
- `--center USERNAME` â€” Prioritize this user's following list
- `--skip-if-ever-scraped` â€” Skip accounts with sufficient existing data
- `--max-scrolls N` â€” Max stagnant scrolls before stopping (default: 6, use 20+ for 1000+ following)
- `--enable-api-fallback` â€” Use X API to enrich accounts with missing bios (slow, rate-limited)
- `--auto-confirm-first` â€” Skip preview confirmation
- `--quiet` â€” Minimal console output, full DEBUG to disk

**Logging:** All runs write DEBUG logs to `logs/enrichment.log` by default. Console shows INFO level unless `--quiet` is used.

In the explorer UI, toggle "Shadow enrichment" to show or hide shadow nodes. Shadow edges render as dashed slate lines; tooltips and detail cards display provenance plus scrape metadata.

## Usage

### Fetching Data

```python
from src.data.fetcher import CachedDataFetcher

with CachedDataFetcher() as fetcher:
    # Fetch profiles (uses cache if fresh, otherwise queries Supabase)
    profiles = fetcher.fetch_profiles()
    print(f"Loaded {len(profiles)} profiles")

    # Force refresh from Supabase
    fresh_profiles = fetcher.fetch_profiles(force_refresh=True)

    # Check cache health
    status = fetcher.cache_status()
    for table, info in status.items():
        print(f"{table}: {info['row_count']} rows, {info['age_days']:.1f} days old")
```

### Cache Management

- **View cache status:** `python3 scripts/verify_setup.py`
- **Force refresh:** Pass `force_refresh=True` to any fetch method
- **Bulk sync:** `python -m scripts.sync_supabase_cache` (add `--force` to bypass cache age checks)
- **Clear cache:** Delete `data/cache.db` and re-run fetcher
- **Adjust freshness:** Set `CACHE_MAX_AGE_DAYS` in `.env`

### Graph Snapshot Management

The API and frontend use precomputed graph snapshots for fast startup (see [ADR 004](./docs/adr/004-precomputed-graph-snapshots.md)).

**Refresh the snapshot after enrichment:**

```bash
python -m scripts.refresh_graph_snapshot --include-shadow
```

This generates:
- `data/graph_snapshot.nodes.parquet` â€” node table
- `data/graph_snapshot.edges.parquet` â€” edge table
- `data/graph_snapshot.meta.json` â€” freshness manifest
- `graph-explorer/public/analysis_output.json` â€” frontend JSON

**Automated refresh:** Add `--refresh-snapshot` to enrichment commands:

```bash
python -m scripts.enrich_shadow_graph --center adityaarpitha --refresh-snapshot
```

**Verify snapshot health:**

```bash
python -m scripts.verify_graph_snapshot
```

**Update README data snapshot (legacy):**

```bash
python -m scripts.analyze_graph --include-shadow --update-readme
```

## Testing

```bash
# Canonical local entrypoints (always use project .venv interpreter)
make verify-louvain-contract
make test-smoke
make test

# Run all unit tests (fast)
.venv/bin/python -m pytest tests/ -v -m unit

# Run integration tests (includes Selenium, slower)
.venv/bin/python -m pytest tests/ -v -m integration

# Run with coverage report
.venv/bin/python -m pytest --cov=src --cov-report=term-missing tests/

# Run specific test file
.venv/bin/python -m pytest tests/test_shadow_enrichment_integration.py -v
```

Test coverage (~68% overall, see `docs/test-coverage-baseline.md` for module-level stats):
- âœ… Supabase connectivity, authentication, and cache expiry safeguards
- âœ… DataFrame schema validation and network error handling
- âœ… Shadow enrichment policy logic (age/delta triggers, skip behavior)
- âœ… Profile extraction (Selenium selector parsing, JSON-LD fallback)
- âœ… Hybrid shadow store persistence (retry logic, coverage conversion)
- âœ… Flask API endpoints powering the graph explorer
- ğŸ§ª 191 pytest cases spanning unit, integration, and Selenium parsing suites

Test suite follows the behavioral testing principles captured in `docs/test-coverage-baseline.md`.

## Development Workflow

1. **Make changes** to `src/` modules
2. **Run tests:** `pytest tests/ -v`
3. **Verify integration:** `python3 scripts/verify_setup.py`
4. **Document decisions:** Add ADRs to `docs/adr/` for architectural changes
5. **Update WORKLOG:** Log progress in `docs/WORKLOG.md`

## Project Structure

```
tpot-analyzer/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ cache.db                  # SQLite cache (gitignored)
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ BACKEND_IMPLEMENTATION.md
â”‚   â”œâ”€â”€ DATABASE_SCHEMA.md
â”‚   â”œâ”€â”€ ENRICHMENT_FLOW.md
â”‚   â”œâ”€â”€ ROADMAP.md
â”‚   â”œâ”€â”€ WORKLOG.md
â”‚   â””â”€â”€ adr/
â”‚       â”œâ”€â”€ 001-data-pipeline-architecture.md
â”‚       â”œâ”€â”€ 002-graph-analysis-foundation.md
â”‚       â””â”€â”€ 003-backend-api-integration.md
â”œâ”€â”€ graph-explorer/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ src/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ analyze_graph.py
â”‚   â”œâ”€â”€ enrich_shadow_graph.py
â”‚   â”œâ”€â”€ verify_shadow_graph.py
â”‚   â””â”€â”€ verify_setup.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ server.py
â”‚   â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ graph/
â”‚   â”œâ”€â”€ shadow/
â”‚   â”œâ”€â”€ ui/
â”‚   â””â”€â”€ logging_utils.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_api.py
â”‚   â”œâ”€â”€ test_shadow_enrichment_integration.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ .env.example
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## Troubleshooting

### "SUPABASE_KEY is not configured"
Export the anon key in your shell or add it to `.env`:
```bash
export SUPABASE_KEY=your_key_here
```

### "Supabase connection failed"
- Check network connectivity
- Verify the anon key is valid and has read access to the Community Archive
- Confirm `SUPABASE_URL` points to the correct instance

### Cache keeps refreshing
- Check `CACHE_MAX_AGE_DAYS` setting (default 7 days)
- Inspect cache metadata: `python3 scripts/verify_setup.py`
- Ensure `data/cache.db` is writable and not being deleted

### Import errors
Activate the virtual environment:
```bash
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

## Roadmap

- [x] **Phase 1.1:** Data pipeline with cached Supabase access âœ…
- [x] **Phase 1.2:** Graph builder, metrics computation, CLI analysis tool âœ…
- [x] **Phase 1.3:** Interactive visualization with React + d3-force âœ…
- [ ] **Phase 1.4:** Shadow enrichment with policy-driven refresh (in progress, 91 tests passing)
- [ ] **Phase 2:** Temporal analysis (growth patterns, community evolution, edge history tracking)
- [ ] **Phase 3:** Advanced metrics (heat diffusion, GNN embeddings, engagement analysis)

## Documentation Guide

- **[README.md](./README.md)** (this file) â€” Project overview, setup, architecture, and usage
- **[CENTER_USER_FIX.md](./CENTER_USER_FIX.md)** â€” Fixes for center user prioritization and Twitter DOM changes (Oct 7-8, 2025)
- **[BUGFIXES.md](./BUGFIXES.md)** â€” Graph Explorer MVP bug fixes (Oct 7, 2025)
- **[TEST_MODE.md](./TEST_MODE.md)** â€” API server test mode for fast UI development
- **[docs/BACKEND_IMPLEMENTATION.md](./docs/BACKEND_IMPLEMENTATION.md)** â€” Flask backend implementation summary (Option B)
- **[docs/WORKLOG.md](./docs/WORKLOG.md)** â€” Detailed development log
- **[docs/ROADMAP.md](./docs/ROADMAP.md)** â€” Forward-looking backlog (testing, features, infra)
- **[docs/adr/](./docs/adr/)** â€” Architectural decision records

## References

- [Community Archive Supabase Schema](https://fabxmporizzqflnftavs.supabase.co)
- [ADR 001: Data Pipeline Architecture](./docs/adr/001-data-pipeline-architecture.md)
- [ADR 002: Graph Analysis & Interactive Exploration](./docs/adr/002-graph-analysis-foundation.md)
- [NetworkX Documentation](https://networkx.org/documentation/stable/)

## License

No license file is provided. Add one before distributing or open-sourcing the project.

#### Cookie Sessions

Run `python -m scripts.setup_cookies` to capture each account's cookies. The script now stores sessions as `secrets/twitter_cookies_<label>_<timestamp>.pkl`. When you launch enrichment without `--cookies`, youâ€™ll be prompted to pick from the available files (the first file is used automatically in `--quiet` mode).

#### Profile-only Backfill

Use `python -m scripts.enrich_shadow_graph --profile-only --delay-min 5 --delay-max 40` to backfill profile metadata for seeds that already have follower/following edges. Pass `--profile-only-all` if you really want to refresh every seed regardless of status.

## Cluster View quickstart

- Precompute artifacts (small fixture example):
  ```bash
  PYTHONPATH=tpot-analyzer python3 tpot-analyzer/scripts/build_spectral.py \
    --data-dir tpot-analyzer/data \
    --output-prefix tpot-analyzer/data/graph_snapshot \
    --limit-nodes 500 --n-dims 10 --maxiter 500 --tol 1e-6
  ```
- Verify artifacts:
  ```bash
  PYTHONPATH=tpot-analyzer python3 tpot-analyzer/scripts/verify_clusters.py \
    --base-path tpot-analyzer/data/graph_snapshot --granularity 25
  ```
- Medium fixture (2000 nodes) lives at `tests/fixtures/medium_graph/graph_snapshot.*` and is exercised in CI.
- Start backend after precompute: `PYTHONPATH=tpot-analyzer python3 tpot-analyzer/scripts/start_api_server.py`
- Frontend: use the Cluster View tab (in `graph-explorer`) to hit `/api/clusters` with granularity and Louvain weight slider.
